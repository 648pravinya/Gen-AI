# -*- coding: utf-8 -*-
"""streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WBzOdUIg1UhRVEPtVzGG7cGV8sjskM-J
"""

!pip install streamlit transformers pyngrok --quiet

from pyngrok import ngrok
ngrok.set_auth_token("2w4N4nACQA8AGYegoiyxgld172O_ffX9SdkokFkbtDPat7sa")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from transformers import pipeline
# 
# # Initialize the QA pipeline
# @st.cache_resource
# def load_model():
#     return pipeline("question-answering", model="deepset/roberta-base-squad2")
# 
# qa_pipeline = load_model()
# 
# # Streamlit UI
# st.title("Question Answering App ðŸ¤–")
# st.write("Enter a context and a question, and get the answer using a pre-trained QA model.")
# 
# # Input fields for context and question
# context = st.text_area("Enter context:", "Hugging Face was founded in 2016 and is known for its open-source NLP models.")
# question = st.text_input("Enter your question:", "When was Hugging Face founded?")
# 
# # Display the answer when the button is clicked
# if st.button("Get Answer"):
#     with st.spinner("Getting answer..."):
#         result = qa_pipeline(question=question, context=context)
#         st.success(f"*Answer:* {result['answer']}")

from pyngrok import ngrok

# Create a public tunnel to port 8501
public_url = ngrok.connect(8501, "http")
print(f"Your Streamlit app is live at: {public_url}")

# Run Streamlit app in the background
!streamlit run app.py &>/content/logs.txt &

